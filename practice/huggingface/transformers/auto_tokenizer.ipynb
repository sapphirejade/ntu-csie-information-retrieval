{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 8667,  146,  112,  182,  170, 1423, 5650,  102],\n",
       "        [ 101, 1262, 1330, 5650,  102,    0,    0,    0,    0],\n",
       "        [ 101, 1262, 1103, 1304, 1304, 1314, 1141,  102,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0]])}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer  #还有其他与模型相关的tokenizer，如BertTokenizer\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained('bert-base-cased') #这里使用的是bert的基础版（12层），区分大小写，实例化一个tokenizer\n",
    "\n",
    "batch_sentences=[\"Hello I'm a single sentence\",\"And another sentence\",\"And the very very last one\"]\n",
    "\n",
    "batch=tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin_result01 =  {'input_ids': tensor([[ 101,  100,  100,  996,  100,  100,  100,  100, 1002,  100, 1031,  100,\n",
      "          102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "origin_result02 =  {'input_ids': tensor([[ 101,  100,  100,  100, 1001,  100,  100,  100,  100,  100,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "origin_result03 =  {'input_ids': tensor([[ 101,  100,  100,  996,  100,  100,  100,  100, 1002,  100, 1031,  100,\n",
      "          102],\n",
      "        [ 101,  100,  100,  100, 1001,  100,  100,  100,  100,  100,  102,    0,\n",
      "            0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "origin_result04 =  [100, 100, 996, 100, 100, 100, 100, 1002, 100, 1031, 100, 100, 100, 100]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "origin_result05 =  [100, 100, 100, 1001, 100, 100, 100, 100, 100]\n",
      "\n",
      " **************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************** \n",
      "\n",
      "auto_result01 =  {'input_ids': tensor([[ 101,  100,  100,  996,  100,  100,  100,  100, 1002,  100, 1031,  100,\n",
      "          102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "auto_result02 =  {'input_ids': tensor([[ 101,  100,  100,  100, 1001,  100,  100,  100,  100,  100,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "auto_result03 =  {'input_ids': tensor([[ 101,  100,  100,  996,  100,  100,  100,  100, 1002,  100, 1031,  100,\n",
      "          102],\n",
      "        [ 101,  100,  100,  100, 1001,  100,  100,  100,  100,  100,  102,    0,\n",
      "            0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "auto_result04 =  [100, 100, 996, 100, 100, 100, 100, 1002, 100, 1031, 100, 100, 100, 100]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "auto_result05 =  [100, 100, 100, 1001, 100, 100, 100, 100, 100]\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "\n",
    "# --------------------------------------- 使用 BertTokenizer ---------------------------------------\n",
    "origin_tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "origin_result01 = origin_tokenizer('对比原始的分词和最新的分词器', padding=True, truncation=True, max_length=13, return_tensors='pt')\n",
    "print(\"origin_result01 = \", origin_result01)\n",
    "print(\"-\" * 100)\n",
    "origin_result02 = origin_tokenizer('展示不同的分词效果', padding=True, truncation=True, max_length=13, return_tensors='pt')\n",
    "print(\"origin_result02 = \", origin_result02)\n",
    "print(\"-\" * 100)\n",
    "origin_result03 = origin_tokenizer(*[['对比原始的分词和最新的分词器', '展示不同的分词效果']], padding=True, truncation=True, max_length=13, return_tensors='pt')\n",
    "print(\"origin_result03 = \", origin_result03)\n",
    "print(\"-\" * 200)\n",
    "\n",
    "origin_result04 = origin_tokenizer.convert_tokens_to_ids(origin_tokenizer.tokenize('对比原始的分词和最新的分词器'))\n",
    "origin_result05 = origin_tokenizer.convert_tokens_to_ids(origin_tokenizer.tokenize('展示不同的分词效果'))\n",
    "\n",
    "print(\"origin_result04 = \", origin_result04)\n",
    "print(\"-\" * 100)\n",
    "print(\"origin_result05 = \", origin_result05)\n",
    "print('\\n', '*' * 400, '\\n')\n",
    "\n",
    "# --------------------------------------- 使用 AutoTokenizer ---------------------------------------\n",
    "auto_tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "auto_result01 = auto_tokenizer('对比原始的分词和最新的分词器', padding=True, truncation=True, max_length=13, return_tensors='pt')\n",
    "print(\"auto_result01 = \", auto_result01)\n",
    "print(\"-\" * 100)\n",
    "auto_result02 = auto_tokenizer('展示不同的分词效果', padding=True, truncation=True, max_length=13, return_tensors='pt')\n",
    "print(\"auto_result02 = \", auto_result02)\n",
    "print(\"-\" * 100)\n",
    "auto_result03 = auto_tokenizer(*[['对比原始的分词和最新的分词器', '展示不同的分词效果']], padding=True, truncation=True, max_length=13, return_tensors='pt')\n",
    "print(\"auto_result03 = \", auto_result03)\n",
    "print(\"-\" * 200)\n",
    "auto_result04 = auto_tokenizer.convert_tokens_to_ids(auto_tokenizer.tokenize('对比原始的分词和最新的分词器'))\n",
    "auto_result05 = auto_tokenizer.convert_tokens_to_ids(auto_tokenizer.tokenize('展示不同的分词效果'))\n",
    "\n",
    "print(\"auto_result04 = \", auto_result04)\n",
    "print(\"-\" * 100)\n",
    "print(\"auto_result05 = \", auto_result05)\n",
    "print(\"-\" * 400)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
