{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISWx1uYQZi8P"
      },
      "source": [
        "# 把它们放在一起 (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUoF1KIBZi8Q"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ifXskIuFZi8R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (2.15.0)\n",
            "Requirement already satisfied: evaluate in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (0.4.1)\n",
            "Requirement already satisfied: transformers in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (4.33.3)\n",
            "Requirement already satisfied: sentencepiece in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (0.1.99)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from datasets) (14.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from datasets) (2.1.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
            "Requirement already satisfied: aiohttp in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages/huggingface_hub-0.19.4-py3.8.egg (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: responses<0.19 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: filelock in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from transformers) (0.11.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets) (4.8.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "import site\n",
        "try:\n",
        "    import datasets\n",
        "    import evaluate\n",
        "    import transformers\n",
        "    import sentencepiece\n",
        "    import accelerate\n",
        "    import torch\n",
        "except ImportError:\n",
        "    %pip install datasets evaluate transformers sentencepiece\n",
        "    %pip install accelerate -U\n",
        "    %pip install torch torchvision torchaudio\n",
        "    %pip install scikit-learn scipy\n",
        "    site.main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Jn1A4qp6Zi8R"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/morris/Documents/code/ntu-csie-information-retrieval/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "model_inputs = tokenizer(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-xdtTMZVZi8R"
      },
      "outputs": [],
      "source": [
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "model_inputs = tokenizer(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NVKbPL7ZZi8S"
      },
      "outputs": [],
      "source": [
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "model_inputs = tokenizer(sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cokGotFsZi8S"
      },
      "outputs": [],
      "source": [
        "# Will pad the sequences up to the maximum sequence length\n",
        "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
        "\n",
        "# Will pad the sequences up to the model max length\n",
        "# (512 for BERT or DistilBERT)\n",
        "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
        "\n",
        "# Will pad the sequences up to the specified max length\n",
        "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eX5uZ9RqZi8S"
      },
      "outputs": [],
      "source": [
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "# Will truncate the sequences that are longer than the model max length\n",
        "# (512 for BERT or DistilBERT)\n",
        "model_inputs = tokenizer(sequences, truncation=True)\n",
        "\n",
        "# Will truncate the sequences that are longer than the specified max length\n",
        "model_inputs = tokenizer(sequences, max_length=8, truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Z6bWGy4IZi8T"
      },
      "outputs": [],
      "source": [
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "# Returns PyTorch tensors\n",
        "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "# Returns TensorFlow tensors\n",
        "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n",
        "\n",
        "# Returns NumPy arrays\n",
        "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2ZncEIleZi8U",
        "outputId": "674c8aae-9333-4f59-b7ad-6deafeb732ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n",
            "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n"
          ]
        }
      ],
      "source": [
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "model_inputs = tokenizer(sequence)\n",
        "print(model_inputs[\"input_ids\"])\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "C0yA3BbHZi8U",
        "outputId": "fd922368-ee2b-4293-a9c8-7f68d1d82ae7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CLS] i've been waiting for a huggingface course my whole life. [SEP]\n",
            "i've been waiting for a huggingface course my whole life.\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
        "print(tokenizer.decode(ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AYXCNtB-Zi8V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "output = model(**tokens)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "把它们放在一起 (PyTorch)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
